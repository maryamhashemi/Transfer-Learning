\section{ادبیات موضوع}
{
	مسئله انتقال یادگیری براساس معیارهای زیادی دسته‌بندی می‌شود. یکی از این دسته‌بندی‌ها براساس پایداری فضای ویژگی‌ها و فضای برچسب‌های دامنه مبدا و هدف است. اگر 
	 $X_s = X_t$ 
	 و 
	 $Y_s=Y_t$
	 در مسئله ما برقرار باشد؛ به این مسئله یک مسئله انتقال یادگیری همگون
	 \footnote{\lr{homogeneous transfer learning}}
	  می‌گوییم و اگر 
	  $X_s \neq X_t$ 
	 و 
	 $Y_s \neq Y_t$
	 برقرار باشد؛ یک مسئله انتقال یادگیری ناهمگون
	 \footnote{\lr{heterogeneous transfer learning}}
	  می‌گوییم. 
	 هر دو روش معرفی شده در 
	 \cite{wang2017balanced}
	 و
	 \cite{wang2019easy}
	 جز مسائل انتقال یادگیری همگون هستند که مبتنی بر فضای ویژگی حل می‌شوند. از این رو در این بخش به بررسی مهمترین روش‌های مبتنی بر فضای ویژگی می‌پردازیم.
	 
	 روش 
	 \lr{\textit{TCA}}
	 \footnote{\lr{Transfer Component Analysis}}
	 تمرکزش را بر روی به حداقل رساندن فاصله توزیع حاشیه‌ای بین دامنه‌ها گذاشته است. در این روش پارامترهای زیادی برای تنظیم کردن وجود دارد. به همین علت از لحاظ محاسباتی هزینه زیادی دارد. از طرفی چون فقط به توزیع حاشیه‌ای پرداخته است و اهمیتی برای توزیع شرطی قائل نشده است؛ عملکرد خوبی را در مسائل واقعی از خود نشان نمی‌دهد
	 \cite{5640675}.
	 روش
	 \lr{\textit{JDA}}
	 \footnote{\lr{Joint Distribution Adaptation}}
	 برای تطبیق دامنه از هر دو توزیع حاشیه‌ای و توزیع شرطی استفاده می‌کند اما اهمیت هر دو توزیع را یکسان در نظر می‌گیرد و خاصیت تعمیم‌دهی در مسائل کاربردی و واقعی را ندارد
	 \cite{6751384}.
	 روش
	 \lr{\textit{GFK}}
	 \footnote{\lr{Geodesic Flow Kernel}}
     یک روش مبتنی بر کرنل است که به بهره برداری از ساختارهای کم بعدی موجود در داده می‌پردازد. رویکردی که این مدل دارد از نظر محاسباتی سودمند است زیرا تنها یک پارامتر دارد که نیاز به تنظیم دارد و آن پارامتر هم تعداد بعدهای ویژگی است. اما این روش زمانی که فاصله‌ی توزیع‌های حاشیه‌ای زیاد است عملکرد خوبی ندارد
     \cite{6247911}.
}

