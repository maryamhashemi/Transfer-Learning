\section{شرح روش پیشنهادی مقالات}
{ 	
	در این بخش ابتدا مسئله تطبیق دامنه را به صورت ریاضی بیان می‌کنیم. سپس مشکلات و چالش‌هایی که در حل این مسئله وجود دارد را بیان خواهیم کرد. در ادامه نیز توضیح خواهیم داد که هر کدام از مقاله‌ها چه روشی را برای حل این مشکلات و چالش‌ها ارائه داده‌اند.
	
	\subsection{تعریف ریاضی مسئله تطبیق دامنه}
	
	ما یک دامنه‌ی برچسب خورده به نام دامنه مبدا و یک دامنه برچسب نخورده به نام دامنه هدف داریم که به ترتیب به  صورت 
$\left\{x_{s_i},\ y_{s_i}\right\}_{i=1} ^ n $
و 
$\left\{x_{t_j}\right\}_{j=1} ^ m$
تعریف می‌شود. همچینین فرضیاتی را در مورد فضای ویژگی‌ها
$X_s = X_t$
و فضای برچسب‌ها
$Y_s=Y_t$
و احتمال حاشیه‌ای  
${P_s(x}_s)\neq{P_t(x}_t)$
و احتمال شرطی
${P_s(y_s|x}_s)\neq{P_t(y_t|x}_t)$
در نظر می‌گیریم. بنابراین هدف انتقال یادگیری این است که فضای برچسب‌های دامنه هدف 
$Y_t$
را به کمک دامنه مبدا
$D_s$
 یاد بگیرد. تطبیق دامنه تلاش می‌کند که با کاهش واگرایی احتمال‌های حاشیه‌ای و احتمال‌های شرطی  بین دامنه‌ی مبدا و هدف، مسئله انتقال یاد گیری را حل کند به عبارت دیگر واگرایی بین 1)
 ${P_s(x}_s)$
 و 
 ${P_t(x}_t)$
 ، 2) 
 ${P_s(y_s|x}_s)$
 و
 ${P_t(y_t|x}_t)$
 را به حداقل می‌رساند.
	
	\subsection{
		تطبیق  متوازن دامنه در یادگیری انتقال
		\protect \footnote{\lr{Balanced Distribution Adaptation for Transfer Learning}} 
	}
	{
		بسیاری از روش‌های تطبیق توزیع موجود، یکی از توزیع‌های حاشیه‌ای یا شرطی و یا هر دو را تطبیق می‌دهند. در مقاله‌های اخیر ثابت شده است که استفاده از هر دو توزیع عملکرد بهتری را نتیجه می‌دهد. اما روش‌های کنونی تاثیر هر دو توزیع را یکسان در نظر می‌گیرند در حالی که وقتی مجموعه داده‌ها بسیار متفاوت باشند، به این معنی است که توزیع حاشیه‌ای اهمیت بیشتری دارد و وقتی مجموعه داده‌ها مشابه هستند، به این معنی است که توزیع شرطی به توجه بیشتری نیاز دارد. از این رو، لازم است از هر دو توزیع با وزنی مناسب با اهمیت آن‌ها برای تطبیق استفاده نمود. علاوه بر این، نامتوازن بودن مجموعه‌داده بر روی تطبیق دامنه تاثیرگذار است که در روش‌های موجود این موضوع در نظر گرفته نمی‌شود. برای حل این دو مشکل، در
\cite{wang2017balanced}
دو روش پیشنهاد شده است. روش اول BDA است که نه تنها می‌تواند توزیع‌های حاشیه‌ای و شرطی بین حوزه‌ها را تطبیق دهد، بلکه اهمیت این دو توزیع را به صورت متوازن تنظیم می‌کند. روش دوم
\lr{W-BDA}
است که مشکل نامتوازن بودن مجموعه داده را حل می‌کند.
\lr{W-BDA}
 می‌تواند هنگام انجام تطبیق توزیع، وزن هر کلاس را به طور انطباقی تغییر دهد.
 	\subsection{BDA}
 	{
 		همانطور که توضیح دادیم در تطبیق دامنه، با کاهش واگرایی یا فاصله ی بین احتمال های حاشیه ای و شرطی می توانیم دامنه های مبدا و هدف را تطبیق دهیم. این موضوع را میتوانیم به صورت عبارت زیر بیان کنیم:
 		\begin{equation}
 			Dist\left(D_{s,}D_t\right)\approx Dist{(P\ (x}_s),{P(x}_t))+Dist({P(y_s|x}_s),{P(y_t|x}_t))
 			\label{eq:1}
 		\end{equation}
 		روش BDA به عبارت 
 		\ref{eq:1}
 		 یک ضریب توازن به نام $\mu$ اضافه می کند که میزان اهمیت توزیع حاشیه ای و شرطی را در مسائل مختلف تنظیم و کنترل می کند. پس عبارت 
 		 \ref{eq:1}
 		 تبدیل می شود به:
 		 \begin{equation}
 			Dist\left(D_{s,}D_t\right)\approx(1-\mu)Dist{(P\ (x}_s),{P(x}_t))+\mu Dist({P(y_s|x}_s),{P(y_t|x}_t))
 			\label{eq:2}
 		 \end{equation}
 		  برای حل عبارت 
 		  \ref{eq:2}
 		  از روش MMD برای تخمین توزیع های حاشیه ای و توزیع های شرطی استفاده می کنیم. بنابراین خواهیم داشت:
 		  \begin{equation}
 		  	 D \left( D_{s,}D_{t} \right)  \approx  \left( 1- \mu  \right)  \bigg \|\frac{1}{n} \sum _{i=1}^{n}x_{s_{i}} - \frac{1}{m} \sum _{j=1}^{m}x_{t_{j}}\bigg \| _{H}^{2} + \mu \sum _{c=1}^{C} \bigg \Vert \frac{1}{n_{c}} \sum _{x_{s_{i}} \in  D_{s}^{ \left( c \right) }}^{n}x_{s_{i}} - \frac{1}{m_{c}} \sum _{x_{t_{j}} \in  D_{t}^{ \left( c \right) }}^{n}x_{t_{j}} \bigg \Vert _{H}^{2}
 		  	 \label{eq:3}
 		  \end{equation}
 		  
 		  در عبارت 
 		 \ref{eq:3}
 		  ترم اول فاصله ی توزیع های حاشیه ای بین دامنه ها را نشان می دهد و ترم دو م هم  فاصله ی توزیع های شرطی بین دامنه ها را نمایش می دهد. با بهره گیری از تکنیک های ماتریس و جبر خطی عبارت 
 		 \ref{eq:3}
 		  را می توان به صورت زیر نوشت:
 		  \begin{equation}
	 	  \begin{aligned}
		  min \quad tr \left( A^{T}X \left( \left( 1 - \mu \right) M_{0} + mu \sum _{c=1\mathrm{ }}^{C}M_{c} \right) X^{T}A \right) + \lambda \Vert A \Vert_{F}^{2} \\
		  s.t. \quad A^{T}XHX^{T}A = I, \quad  0  \leq \mu \leq  1 \quad \quad \quad \quad \quad
		  \label{eq:4}
	 	  \end{aligned}
 		  \end{equation}
 		   در عبارت
 		   \ref{eq:4}
 		    دو ترم وجود دارد: 1) تطبیق توزیع های حاشیه ای و شرطی به همراه ضریب توازن $\mu$ 2) ترم مربوط به رگلاریسیون با ضریب رگلارسیون $\lambda$. در این عبارت 2 قید هم داریم. قید اول بیان می کند که داده ی تغییر یافته ویژگی های داخلی اصل داده را حفظ می کند. قید دوم نیز بازه ی ضریب توازن را بیان می کند.
 		    
 		    برای حل عبارت 
 		    \ref{eq:4}
 		    از ضرائب لاگرانژ استفاده می کنیم. بنابراین اگر ضرائب لاگرانژ باشد، خواهیم داشت: 
 		    \begin{equation}
 		    	\begin{aligned}
 		    	L = tr \left( A^{T}X \left( \left( 1 - \mu \right) M_{0} + mu \sum _{c=1\mathrm{ }}^{C}M_{c} \right) X^{T}A \right) + \lambda \Vert A \Vert_{F}^{2} + tr \left( \left( I -  A^{T}XHX^{T}A)\right) \phi \right) 
 		    	\label{eq:5}
 		    	\end{aligned}
 		    \end{equation}
 		 
 	}
	}

	
	
	
	
	\subsection{
		انتقال یادگیری آسان با استفاده از ساختارهای درون دامنه
		\protect \footnote{\lr{Easy Transfer Learning By Exploiting Intra-domain Structures}}
	}
}




